{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from ClassificationReport import ClassificationMetrics\n",
    "\n",
    "import ClassificationReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate precision. It's important for measuring exactness.\n",
    "def calculate_precision(tp, fp):\n",
    "    if (tp + fp) > 0:\n",
    "        return tp / (tp + fp)  # True Positives divided by Total Predicted Positives\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Function to calculate recall, which measures how well we capture all positives\n",
    "def calculate_recall(tp, fn):\n",
    "    if (tp + fn) > 0:\n",
    "        return tp / (tp + fn)  # True Positives divided by Actual Total Positives\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# This function calculates F1 score, which is the harmonic mean of precision and recall\n",
    "def calculate_f1_score(precision, recall):\n",
    "    if (precision + recall) > 0:\n",
    "        return 2 * (precision * recall) / (precision + recall)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Here ,we calculate the accuracy of our model\n",
    "def calculate_accuracy(true_labels, pred_labels):\n",
    "    correct_predictions = np.sum(true_labels == pred_labels)\n",
    "    total_predictions = len(true_labels)\n",
    "    return correct_predictions / total_predictions  # Correct predictions over all predictions\n",
    "\n",
    "# This function builds the confusion matrix, useful for visualizing TP, FP, FN, TN\n",
    "def confusion_matrix(true_labels, pred_labels, classes):\n",
    "    class_index = {k: i for i, k in enumerate(classes)}  # Create a map of class to index\n",
    "    matrix = np.zeros((len(classes), len(classes)), dtype=int)  # Init matrix of zeros\n",
    "    for true, pred in zip(true_labels, pred_labels):\n",
    "        true_idx = class_index[true]\n",
    "        pred_idx = class_index[pred]\n",
    "        matrix[true_idx][pred_idx] += 1  # Increment the cell for each prediction\n",
    "    return matrix\n",
    "\n",
    "# Generate and print out the classification report\n",
    "def classification_report_categorical(true_labels, pred_labels):\n",
    "    classes = np.unique(np.concatenate([true_labels, pred_labels]))  # Getting all classes\n",
    "    cm = confusion_matrix(true_labels, pred_labels, classes)\n",
    "    accuracy = calculate_accuracy(true_labels, pred_labels)  # Overall accuracy\n",
    "    print(f\"{'Class':<30}{'Precision':<10}{'Recall':<10}{'F1-Score':<10}\") \n",
    "    for i, class_name in enumerate(classes):\n",
    "        tp = cm[i, i]\n",
    "        fp = np.sum(cm[:, i]) - tp\n",
    "        fn = np.sum(cm[i, :]) - tp\n",
    "        precision = calculate_precision(tp, fp)\n",
    "        recall = calculate_recall(tp, fn)\n",
    "        f1_score = calculate_f1_score(precision, recall)\n",
    "        # Printing each class with metrics\n",
    "        print(f\"{class_name:<30}{precision:<10.2f}{recall:<10.2f}{f1_score:<10.2f}\")\n",
    "    print(f\"\\n{'Overall Accuracy:':<30}{accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "X = np.load('../data/processed/X.npy')\n",
    "y = np.load('../data/processed/y.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (563, 57915)\n",
      "Shape of y: (563,)\n",
      "Unique types: ['ER+' 'ER+HER2+' 'ER+HER2+ LN metastasis' 'HER2+' 'TNBC'\n",
      " 'TNBC LN metastasis']\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)\n",
    "\n",
    "# list all unique subtypes\n",
    "unique_types = np.unique(y)\n",
    "print(\"Unique types:\", unique_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.7079646  0.84070796 0.84070796 0.83928571 0.61607143]\n",
      "Mean CV Score: 0.7689475347661189\n",
      "Class                         Precision Recall    F1-Score  \n",
      "ER+                           1.00      1.00      1.00      \n",
      "ER+HER2+                      0.75      0.50      0.60      \n",
      "ER+HER2+ LN metastasis        0.78      0.70      0.74      \n",
      "HER2+                         0.96      0.89      0.92      \n",
      "TNBC                          0.76      0.97      0.85      \n",
      "TNBC LN metastasis            0.83      0.45      0.59      \n",
      "\n",
      "Overall Accuracy:             0.86\n"
     ]
    }
   ],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "cv_scores = cross_val_score(rf_classifier, X_scaled, y, cv=5)\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Mean CV Score:\", cv_scores.mean())\n",
    "\n",
    "# Train the classifier on the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing data\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the classifier performance\n",
    "#print(classification_report(y_test, y_pred, num_classes))\n",
    "#classification_report_categorical(y_test, y_pred)\n",
    "metrics = ClassificationMetrics(y_test, y_pred)\n",
    "metrics.report()\n",
    "\n",
    "#print(\"Accuracy on test data: \", accuracy_score(y_test, y_test))\n",
    "#print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "#print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
